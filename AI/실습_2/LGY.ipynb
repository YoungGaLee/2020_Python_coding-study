{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6개월 전_MNIST",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aw5nw5bMnUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe2be489-fd05-4bf4-a725-78439d64587c"
      },
      "source": [
        "#코랩에서 돌아가지 않는 코드입니다... 어떤 에러인지도 안뜨고... 세션이 다운되고 비정상 종료됩니다...\n",
        "# --> data를 한 번에 다 load하는 코드 존재 ==> 서버가 감당하지 못하고 세션 다운 -> DataLoader와 batch를 사용하여 이를 해결\n",
        "\n",
        "#저번 MNIST_CNN_import : 이거 다 없어도 될거 같음\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#변환 코드_import \n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import time\n",
        "\n",
        "#클래스 따로 위쪽으로 뺌.\n",
        "  #변환코드클래스\n",
        "class MyDataset(Dataset):\n",
        "  \n",
        "    def __init__(self, image_path, label_path):\n",
        "        self.image_data = torch.from_numpy(self.read_image(image_path)) #numpy배열 > tensor로 바꿔줌\n",
        "        self.label_data = torch.from_numpy(self.read_label(label_path)).long()\n",
        "        self.len = self.label_data.size()[0]\n",
        "        #__init__부분에서 read_image함수를 불러오고 read_image에서 read함수를 불어오기 때문에\n",
        "        #따로 사용하는 것이 아니라 그냥 MyDataset에 경로지정해주면 알아서 진행\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.image_data[item].float() ,self.label_data[item]\n",
        "\n",
        "\n",
        "    #read라는 함수를 돌릴때마다 앞의 4자리 읽어서 출력하는거 28 또는 0으로 되더라\n",
        "    def read(self, data_file): # 앞의 4byte를 읽은 후 표준 형식으로 바꿔 출력    ????numpy????\n",
        "        dt = np.dtype(np.uint32).newbyteorder('>')#np.dtype: ,numpy.dtype.newbyteorder(uint32)부호화되지 않은 32비트=4바이트 순서로 있는 원래 자료를 해석할 수 있도록 배열 dtype에 바이트 순서 정보를 변경하는 것\n",
        "        # >u4 : 4글자 유니코드 문자열\n",
        "        #what = np.frombuffer(data_file.read(4), dt)\n",
        "        #print(what)\n",
        "        #print(what[0])\n",
        "\n",
        "        return np.frombuffer(data_file.read(4), dt)[0] #np.frombuffer( 바꾸고 싶은 bytes , dtype = <자료형>)\n",
        "\n",
        "    def read_image(self, image_path):\n",
        "        image_file = open(image_path, 'rb')\n",
        "        # <_io.BufferedReader name='/content/drive/My Drive/MNIST_byte/t10k-images.idx3-ubyte'>\n",
        "        image_file.read(4) # 처음 4byte는 데이터가 MNIST라는것을 의미\n",
        "        #'<--- 이 부분은 읽은 후에 저장되지 않고 사라진다.'\n",
        "        # read()함수가 실행되는것이 아님....이거 뭐야? 그냥 네개를 읽어라. <-- 4개를 읽고 버리는 것\n",
        "        # read() : 파일 전체의 내용을 하나의 문자열로 읽어온다.그냥 내장함수인듯...??\n",
        "        #b\"\\x00\\x00'\\x10\"\n",
        "        \n",
        "\n",
        "        \n",
        "        #계속 같은 ubyte파일을 넣어줌\n",
        "        #앞에 이게 파일에서 얼마로 이루어졌다 하는 정보인듯함\n",
        "        num_images = self.read(image_file) #10000 : 처음4개 \n",
        "        #' < -- 4개를 버리지 않고 정보를 저장'\n",
        "        rows = self.read(image_file) # 28 : 그다음4개\n",
        "        cols = self.read(image_file) # 28: 그다다음4개\n",
        "\n",
        "\n",
        "        buf = image_file.read(rows * cols * num_images) # 28*28*10000개(전체 수) 읽기(불러오기) = buf:test_image파일에 있는 전체 내용을 담고 있음.\n",
        "        #다 읽어왔으니까 close\n",
        "        image_file.close()\n",
        "        \n",
        "\n",
        "        data = np.frombuffer(buf, np.uint8) # ?? 왜 다시 이걸로 바꾸지..? 1바이트로   ????????? \n",
        "        #' <-- np.frombuffer는 1항에 데이터 2항에 타입으로 1항의 값을 2항의 타입으로 바꾸는 함수, uint8은 0~255 로 8bit인 int형 데이터 타입'\n",
        "        data = data.reshape(num_images, 1, rows, cols) #data : (10000,1,28,28)로 reshape \n",
        "        #'<- 위에서 바꿔 읽은 1차원 데이터를 원하는 형테로 바꿔줌'\n",
        "        # print(data)#array\n",
        "        return data\n",
        "\n",
        "    def read_label(self, label_path):\n",
        "        label_file = open(label_path, 'rb')\n",
        "        label_file.read(4)# 처음 4byte는 데이터가 MNIST라는것을 의미(동일) / self없으니까 그냥 python 내장함수인 read()함수 사용 \n",
        "        #'<---처음 4개 데이터는 읽어온 후 어디에도 저장하지 않고 버림'\n",
        "        num_label = self.read(label_file)#self.read니까 class에 있는 read함수 사용 /전체를 4byte로 바꿈?  ?????딱 4개만 바꾼거 아닌가????\n",
        "         #'<- 그 다음 4개 데이터를 읽은 후 원하는 형태로 바꿔 num_label에 저장, 저장 된 값은 int형'\n",
        "        # print(\"여기 지나가긴 하니?\")\n",
        "        # print(num_label) #10000\n",
        "        # print(\"이거야?\")\n",
        "\n",
        "        # print(type(label_file)) #buffer reader 처리 된거\n",
        "        # print(type(num_label))\n",
        "\n",
        "\n",
        "        buf = label_file.read(num_label) # 10000개 읽어옴.\n",
        "        label_file.close()\n",
        "\n",
        "        labels = np.frombuffer(buf, np.uint8)\n",
        "        return labels\n",
        "\n",
        "\n",
        "\n",
        "# 2개 레이어 CNN 클래스\n",
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
        "        #    Conv     -> (?, 28, 28, 32)\n",
        "        #    Pool     -> (?, 14, 14, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
        "        #    Conv      ->(?, 14, 14, 64)\n",
        "        #    Pool      ->(?, 7, 7, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # Final FC 7x7x64 inputs -> 10 outputs\n",
        "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:,0,:,:]\n",
        "        w,h = x.shape[1],x.shape[2]\n",
        "        x = x.view(-1,1,w,h)\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "  \n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "#기본 설정\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "epochs = 5\n",
        "batch_size = 200 #ubyte를 바꾸는거니까 용량상관없으니 안써도 되지 않을까? \n",
        " #'<-- ubyte를 로드하면서 numpy 이미지로 바꾸기 때문에 용량이 매우 커짐.'\n",
        "\n",
        "#경로지정\n",
        "Image_path_for_test = '/content/drive/My Drive/MNIST_byte/t10k-images.idx3-ubyte' # test_X\n",
        "Label_path_for_test = '/content/drive/My Drive/MNIST_byte/t10k-labels.idx1-ubyte'  # test_Y\n",
        "\n",
        "image_path_for_train = '/content/drive/My Drive/MNIST_byte/train-images.idx3-ubyte'  # train_X\n",
        "Label_path_for_train = '/content/drive/My Drive/MNIST_byte/train-labels.idx1-ubyte' # train_Y\n",
        "\n",
        "\n",
        "#클래스사용\n",
        "dataset_for_test = MyDataset(Image_path_for_test,Label_path_for_test)\n",
        "dataset_for_train = MyDataset(image_path_for_train,Label_path_for_train)\n",
        "model = CNN().to(device)\n",
        "\n",
        "\n",
        "'''  여기가 문제의 부분\n",
        "--> 모든 데이터를 한 번에 numpy로 바꾸어 load\n",
        "--> 그 데이터를 후에 .to(device)로 gpu에 올리게 되는데 일반 ram과 video_ram은 다름\n",
        "--> cuda는 video_ram을 사용하며 80~160만원 사이의 gpu가 보통 video_ram이 8~11GB\n",
        "\n",
        "train_X = dataset_for_train.image_data\n",
        "train_Y = dataset_for_train.label_data\n",
        "\n",
        "test_X = dataset_for_test.image_data\n",
        "test_Y = dataset_for_train.label_data\n",
        "'''\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset_for_train, batch_size=batch_size)\n",
        " #'<-- DataLoader를 이용하여 원하는 batch_size만큼 load할 준비'\n",
        "num_batches = len(train_loader) // batch_size\n",
        "\n",
        "test_loader = DataLoader(dataset_for_test, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "# loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# training\n",
        "print('Learning started. It takes sometime.')\n",
        "for epoch in range(epochs): # epoch먼저\n",
        "    for batch, (train_X, train_Y) in enumerate(train_loader, 1): \n",
        "      #'위에서 준비한 train_loader를 이용하여 데이터를 load'\n",
        "      avg_cost = 0\n",
        "      # image is already size of (28x28), no reshape\n",
        "      # label is not one-hot encoded\n",
        "      X = train_X.to(device)\n",
        "      Y = train_Y.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      hypothesis = model(X)\n",
        "      cost = criterion(hypothesis, Y)\n",
        "      cost.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      avg_cost += cost / num_batches\n",
        "\n",
        "      if batch % 10== 0:\n",
        "          print('[Epoch: {:>4}] batch: {:>4}  cost = {:>.9}'.format(epoch + 1, batch, cost.item()))\n",
        "\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "# with torch.no_grad():\n",
        "#     X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "#     Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "#     prediction = model(X_test)\n",
        "#     correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "#     accuracy = correct_prediction.float().mean()\n",
        "#     print('Accuracy:', accuracy.item())\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning started. It takes sometime.\n",
            "[Epoch:    1] batch:   10  cost = 15.6693039\n",
            "[Epoch:    1] batch:   20  cost = 3.32426143\n",
            "[Epoch:    1] batch:   30  cost = 2.490906\n",
            "[Epoch:    1] batch:   40  cost = 1.47011387\n",
            "[Epoch:    1] batch:   50  cost = 0.378743052\n",
            "[Epoch:    1] batch:   60  cost = 1.22485971\n",
            "[Epoch:    1] batch:   70  cost = 0.936605513\n",
            "[Epoch:    1] batch:   80  cost = 0.936311901\n",
            "[Epoch:    1] batch:   90  cost = 0.384843558\n",
            "[Epoch:    1] batch:  100  cost = 0.470407128\n",
            "[Epoch:    1] batch:  110  cost = 0.334894031\n",
            "[Epoch:    1] batch:  120  cost = 0.428058714\n",
            "[Epoch:    1] batch:  130  cost = 0.346244425\n",
            "[Epoch:    1] batch:  140  cost = 0.279209852\n",
            "[Epoch:    1] batch:  150  cost = 0.742539167\n",
            "[Epoch:    1] batch:  160  cost = 0.200391591\n",
            "[Epoch:    1] batch:  170  cost = 0.114396363\n",
            "[Epoch:    1] batch:  180  cost = 0.265994489\n",
            "[Epoch:    1] batch:  190  cost = 0.282021075\n",
            "[Epoch:    1] batch:  200  cost = 0.255541682\n",
            "[Epoch:    1] batch:  210  cost = 0.29065758\n",
            "[Epoch:    1] batch:  220  cost = 0.232934654\n",
            "[Epoch:    1] batch:  230  cost = 0.365722686\n",
            "[Epoch:    1] batch:  240  cost = 0.257088155\n",
            "[Epoch:    1] batch:  250  cost = 0.578696311\n",
            "[Epoch:    1] batch:  260  cost = 0.200648323\n",
            "[Epoch:    1] batch:  270  cost = 0.360333323\n",
            "[Epoch:    1] batch:  280  cost = 0.233039901\n",
            "[Epoch:    1] batch:  290  cost = 0.072470881\n",
            "[Epoch:    1] batch:  300  cost = 0.262478679\n",
            "[Epoch:    2] batch:   10  cost = 0.0451233946\n",
            "[Epoch:    2] batch:   20  cost = 0.0613899194\n",
            "[Epoch:    2] batch:   30  cost = 0.206336498\n",
            "[Epoch:    2] batch:   40  cost = 0.103059486\n",
            "[Epoch:    2] batch:   50  cost = 0.0421603732\n",
            "[Epoch:    2] batch:   60  cost = 0.173210472\n",
            "[Epoch:    2] batch:   70  cost = 0.129904389\n",
            "[Epoch:    2] batch:   80  cost = 0.253309488\n",
            "[Epoch:    2] batch:   90  cost = 0.157051474\n",
            "[Epoch:    2] batch:  100  cost = 0.0538003445\n",
            "[Epoch:    2] batch:  110  cost = 0.101345822\n",
            "[Epoch:    2] batch:  120  cost = 0.270010471\n",
            "[Epoch:    2] batch:  130  cost = 0.0987621769\n",
            "[Epoch:    2] batch:  140  cost = 0.13827455\n",
            "[Epoch:    2] batch:  150  cost = 0.157970905\n",
            "[Epoch:    2] batch:  160  cost = 0.161670148\n",
            "[Epoch:    2] batch:  170  cost = 0.0617857948\n",
            "[Epoch:    2] batch:  180  cost = 0.107924111\n",
            "[Epoch:    2] batch:  190  cost = 0.0574742183\n",
            "[Epoch:    2] batch:  200  cost = 0.0824414939\n",
            "[Epoch:    2] batch:  210  cost = 0.157186329\n",
            "[Epoch:    2] batch:  220  cost = 0.0915875137\n",
            "[Epoch:    2] batch:  230  cost = 0.215829417\n",
            "[Epoch:    2] batch:  240  cost = 0.143858179\n",
            "[Epoch:    2] batch:  250  cost = 0.176999509\n",
            "[Epoch:    2] batch:  260  cost = 0.170281157\n",
            "[Epoch:    2] batch:  270  cost = 0.175103247\n",
            "[Epoch:    2] batch:  280  cost = 0.0892623588\n",
            "[Epoch:    2] batch:  290  cost = 0.00969685335\n",
            "[Epoch:    2] batch:  300  cost = 0.176335603\n",
            "[Epoch:    3] batch:   10  cost = 0.0253769401\n",
            "[Epoch:    3] batch:   20  cost = 0.0291495249\n",
            "[Epoch:    3] batch:   30  cost = 0.0714866221\n",
            "[Epoch:    3] batch:   40  cost = 0.0579737797\n",
            "[Epoch:    3] batch:   50  cost = 0.0177470744\n",
            "[Epoch:    3] batch:   60  cost = 0.0940293223\n",
            "[Epoch:    3] batch:   70  cost = 0.0495320745\n",
            "[Epoch:    3] batch:   80  cost = 0.117335409\n",
            "[Epoch:    3] batch:   90  cost = 0.0923704803\n",
            "[Epoch:    3] batch:  100  cost = 0.0386294797\n",
            "[Epoch:    3] batch:  110  cost = 0.0539275557\n",
            "[Epoch:    3] batch:  120  cost = 0.138923571\n",
            "[Epoch:    3] batch:  130  cost = 0.105295889\n",
            "[Epoch:    3] batch:  140  cost = 0.0842252895\n",
            "[Epoch:    3] batch:  150  cost = 0.103740275\n",
            "[Epoch:    3] batch:  160  cost = 0.085976392\n",
            "[Epoch:    3] batch:  170  cost = 0.0174777582\n",
            "[Epoch:    3] batch:  180  cost = 0.0455970727\n",
            "[Epoch:    3] batch:  190  cost = 0.0346636362\n",
            "[Epoch:    3] batch:  200  cost = 0.0738384649\n",
            "[Epoch:    3] batch:  210  cost = 0.11230655\n",
            "[Epoch:    3] batch:  220  cost = 0.0503098965\n",
            "[Epoch:    3] batch:  230  cost = 0.181174144\n",
            "[Epoch:    3] batch:  240  cost = 0.132412165\n",
            "[Epoch:    3] batch:  250  cost = 0.179667935\n",
            "[Epoch:    3] batch:  260  cost = 0.138235882\n",
            "[Epoch:    3] batch:  270  cost = 0.0699894354\n",
            "[Epoch:    3] batch:  280  cost = 0.0414184667\n",
            "[Epoch:    3] batch:  290  cost = 0.00689537404\n",
            "[Epoch:    3] batch:  300  cost = 0.180500761\n",
            "[Epoch:    4] batch:   10  cost = 0.0171960089\n",
            "[Epoch:    4] batch:   20  cost = 0.0175992157\n",
            "[Epoch:    4] batch:   30  cost = 0.0725294575\n",
            "[Epoch:    4] batch:   40  cost = 0.0353839286\n",
            "[Epoch:    4] batch:   50  cost = 0.0182013884\n",
            "[Epoch:    4] batch:   60  cost = 0.0795785412\n",
            "[Epoch:    4] batch:   70  cost = 0.0501817092\n",
            "[Epoch:    4] batch:   80  cost = 0.0619690195\n",
            "[Epoch:    4] batch:   90  cost = 0.0581964999\n",
            "[Epoch:    4] batch:  100  cost = 0.0312343743\n",
            "[Epoch:    4] batch:  110  cost = 0.0206247568\n",
            "[Epoch:    4] batch:  120  cost = 0.102349184\n",
            "[Epoch:    4] batch:  130  cost = 0.0666858777\n",
            "[Epoch:    4] batch:  140  cost = 0.0383935273\n",
            "[Epoch:    4] batch:  150  cost = 0.0433425382\n",
            "[Epoch:    4] batch:  160  cost = 0.0582500473\n",
            "[Epoch:    4] batch:  170  cost = 0.00765229948\n",
            "[Epoch:    4] batch:  180  cost = 0.0523602404\n",
            "[Epoch:    4] batch:  190  cost = 0.0417283326\n",
            "[Epoch:    4] batch:  200  cost = 0.0360234529\n",
            "[Epoch:    4] batch:  210  cost = 0.0937467664\n",
            "[Epoch:    4] batch:  220  cost = 0.0516385697\n",
            "[Epoch:    4] batch:  230  cost = 0.218795896\n",
            "[Epoch:    4] batch:  240  cost = 0.118888713\n",
            "[Epoch:    4] batch:  250  cost = 0.070023261\n",
            "[Epoch:    4] batch:  260  cost = 0.133344784\n",
            "[Epoch:    4] batch:  270  cost = 0.0346103162\n",
            "[Epoch:    4] batch:  280  cost = 0.0408679619\n",
            "[Epoch:    4] batch:  290  cost = 0.0242231432\n",
            "[Epoch:    4] batch:  300  cost = 0.172395036\n",
            "[Epoch:    5] batch:   10  cost = 0.0166918654\n",
            "[Epoch:    5] batch:   20  cost = 0.00302803307\n",
            "[Epoch:    5] batch:   30  cost = 0.0474697165\n",
            "[Epoch:    5] batch:   40  cost = 0.0315424837\n",
            "[Epoch:    5] batch:   50  cost = 0.0103733242\n",
            "[Epoch:    5] batch:   60  cost = 0.0418492891\n",
            "[Epoch:    5] batch:   70  cost = 0.00778411375\n",
            "[Epoch:    5] batch:   80  cost = 0.0457588919\n",
            "[Epoch:    5] batch:   90  cost = 0.0152391028\n",
            "[Epoch:    5] batch:  100  cost = 0.0102145486\n",
            "[Epoch:    5] batch:  110  cost = 0.0186571702\n",
            "[Epoch:    5] batch:  120  cost = 0.0682511628\n",
            "[Epoch:    5] batch:  130  cost = 0.0666228682\n",
            "[Epoch:    5] batch:  140  cost = 0.0172014087\n",
            "[Epoch:    5] batch:  150  cost = 0.0172554813\n",
            "[Epoch:    5] batch:  160  cost = 0.0242931582\n",
            "[Epoch:    5] batch:  170  cost = 0.0077389474\n",
            "[Epoch:    5] batch:  180  cost = 0.0511519685\n",
            "[Epoch:    5] batch:  190  cost = 0.0340126976\n",
            "[Epoch:    5] batch:  200  cost = 0.019044999\n",
            "[Epoch:    5] batch:  210  cost = 0.0423486121\n",
            "[Epoch:    5] batch:  220  cost = 0.0394443274\n",
            "[Epoch:    5] batch:  230  cost = 0.196509749\n",
            "[Epoch:    5] batch:  240  cost = 0.0451211296\n",
            "[Epoch:    5] batch:  250  cost = 0.0745908692\n",
            "[Epoch:    5] batch:  260  cost = 0.100627281\n",
            "[Epoch:    5] batch:  270  cost = 0.0310360435\n",
            "[Epoch:    5] batch:  280  cost = 0.0271668788\n",
            "[Epoch:    5] batch:  290  cost = 0.0244655386\n",
            "[Epoch:    5] batch:  300  cost = 0.170167163\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqMXX0FPup8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B99jHGLbuqGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
