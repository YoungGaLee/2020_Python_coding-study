{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RomWHbBFP03B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHxvSfJFQbCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "seed = 777\n",
        "learning_rate = 3e-1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpSjfe1gQYCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJAmOf_QgL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXt2CfYeQrtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear = torch.nn.Linear(784, 10, bias=True).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=learning_rate)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52tFUEy7S21n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddf3b2ed-0fc6-441b-bdbb-a642b7bb9e62"
      },
      "source": [
        "best_acc = -1\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = len(data_loader)\n",
        "\n",
        "    for batch, (X, Y) in enumerate(data_loader, 1):\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = linear(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "        if batch % (total_batch//10) == 0:\n",
        "            with torch.no_grad():\n",
        "                ranges = list(range(len(mnist_train)))\n",
        "                incidence = random.sample(ranges, len(mnist_test))\n",
        "                X_train = mnist_train.data[incidence].view(-1, 28 * 28).float().to(device)\n",
        "                Y_train = mnist_train.targets[incidence].to(device)\n",
        "                \n",
        "                X_test = mnist_test.data.view(-1, 28 * 28).float().to(device)\n",
        "                Y_test = mnist_test.targets.to(device)\n",
        "\n",
        "                prediction = linear(X_train)\n",
        "                correct_prediction = torch.argmax(prediction, 1) == Y_train\n",
        "                train_accuracy = correct_prediction.float().mean()\n",
        "\n",
        "                prediction = linear(X_test)\n",
        "                correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "                test_accuracy = correct_prediction.float().mean()\n",
        "\n",
        "                if best_acc < test_accuracy.item():\n",
        "                    best_acc = test_accuracy.item()\n",
        "\n",
        "            print('Epoch: {:02d}\\tBatch: {:03d}\\tcost = {:.9f}\\tTrain_Acc: {:.2f}\\tTest_Acc: {:.2f}\\t\\tBest_Test_Acc: {:.2f}'.format(epoch + 1, batch, cost.item(), train_accuracy.item()*100, test_accuracy.item()*100, best_acc*100))\n",
        "    print('='*120)\n",
        "print('Learning finished')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01\tBatch: 060\tcost = 0.706622005\tTrain_Acc: 86.36\tTest_Acc: 87.31\t\tBest_Test_Acc: 87.31\n",
            "Epoch: 01\tBatch: 120\tcost = 0.463007241\tTrain_Acc: 88.44\tTest_Acc: 89.19\t\tBest_Test_Acc: 89.19\n",
            "Epoch: 01\tBatch: 180\tcost = 0.319952726\tTrain_Acc: 88.86\tTest_Acc: 89.65\t\tBest_Test_Acc: 89.65\n",
            "Epoch: 01\tBatch: 240\tcost = 0.474801898\tTrain_Acc: 88.71\tTest_Acc: 89.57\t\tBest_Test_Acc: 89.65\n",
            "Epoch: 01\tBatch: 300\tcost = 0.515292048\tTrain_Acc: 89.40\tTest_Acc: 90.13\t\tBest_Test_Acc: 90.13\n",
            "Epoch: 01\tBatch: 360\tcost = 0.470868409\tTrain_Acc: 89.20\tTest_Acc: 89.90\t\tBest_Test_Acc: 90.13\n",
            "Epoch: 01\tBatch: 420\tcost = 0.303796589\tTrain_Acc: 89.22\tTest_Acc: 90.35\t\tBest_Test_Acc: 90.35\n",
            "Epoch: 01\tBatch: 480\tcost = 0.385777444\tTrain_Acc: 88.48\tTest_Acc: 89.58\t\tBest_Test_Acc: 90.35\n",
            "Epoch: 01\tBatch: 540\tcost = 0.433931261\tTrain_Acc: 89.54\tTest_Acc: 89.97\t\tBest_Test_Acc: 90.35\n",
            "Epoch: 01\tBatch: 600\tcost = 0.182896808\tTrain_Acc: 88.57\tTest_Acc: 89.91\t\tBest_Test_Acc: 90.35\n",
            "========================================================================================================================\n",
            "Epoch: 02\tBatch: 060\tcost = 0.605541706\tTrain_Acc: 88.22\tTest_Acc: 88.73\t\tBest_Test_Acc: 90.35\n",
            "Epoch: 02\tBatch: 120\tcost = 0.429316670\tTrain_Acc: 89.89\tTest_Acc: 90.54\t\tBest_Test_Acc: 90.54\n",
            "Epoch: 02\tBatch: 180\tcost = 0.275251299\tTrain_Acc: 90.57\tTest_Acc: 90.28\t\tBest_Test_Acc: 90.54\n",
            "Epoch: 02\tBatch: 240\tcost = 0.437528878\tTrain_Acc: 89.77\tTest_Acc: 90.08\t\tBest_Test_Acc: 90.54\n",
            "Epoch: 02\tBatch: 300\tcost = 0.213963374\tTrain_Acc: 89.83\tTest_Acc: 90.54\t\tBest_Test_Acc: 90.54\n",
            "Epoch: 02\tBatch: 360\tcost = 0.333667219\tTrain_Acc: 89.86\tTest_Acc: 90.14\t\tBest_Test_Acc: 90.54\n",
            "Epoch: 02\tBatch: 420\tcost = 0.272356093\tTrain_Acc: 88.50\tTest_Acc: 89.33\t\tBest_Test_Acc: 90.54\n",
            "Epoch: 02\tBatch: 480\tcost = 0.347979724\tTrain_Acc: 90.23\tTest_Acc: 90.75\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 02\tBatch: 540\tcost = 0.413693130\tTrain_Acc: 89.96\tTest_Acc: 89.52\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 02\tBatch: 600\tcost = 0.371059000\tTrain_Acc: 89.88\tTest_Acc: 90.20\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Epoch: 03\tBatch: 060\tcost = 0.244062915\tTrain_Acc: 88.83\tTest_Acc: 89.93\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 120\tcost = 0.165102601\tTrain_Acc: 89.70\tTest_Acc: 90.61\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 180\tcost = 0.451369673\tTrain_Acc: 87.68\tTest_Acc: 88.36\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 240\tcost = 0.374717325\tTrain_Acc: 87.60\tTest_Acc: 88.39\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 300\tcost = 0.380824924\tTrain_Acc: 89.30\tTest_Acc: 89.73\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 360\tcost = 0.204922542\tTrain_Acc: 88.65\tTest_Acc: 88.77\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 420\tcost = 0.368249655\tTrain_Acc: 89.37\tTest_Acc: 89.81\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 480\tcost = 0.225366041\tTrain_Acc: 89.13\tTest_Acc: 89.82\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 540\tcost = 0.353871435\tTrain_Acc: 89.72\tTest_Acc: 90.06\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 03\tBatch: 600\tcost = 0.274257272\tTrain_Acc: 87.22\tTest_Acc: 87.72\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Epoch: 04\tBatch: 060\tcost = 0.227252617\tTrain_Acc: 87.60\tTest_Acc: 88.21\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 120\tcost = 0.342749417\tTrain_Acc: 86.64\tTest_Acc: 87.39\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 180\tcost = 0.155330464\tTrain_Acc: 89.38\tTest_Acc: 89.60\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 240\tcost = 0.195884660\tTrain_Acc: 89.40\tTest_Acc: 89.81\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 300\tcost = 0.262353987\tTrain_Acc: 89.09\tTest_Acc: 89.18\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 360\tcost = 0.380331725\tTrain_Acc: 88.84\tTest_Acc: 88.67\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 420\tcost = 0.235402793\tTrain_Acc: 89.03\tTest_Acc: 89.43\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 480\tcost = 0.521415889\tTrain_Acc: 89.15\tTest_Acc: 88.84\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 540\tcost = 0.283296257\tTrain_Acc: 89.59\tTest_Acc: 89.58\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 04\tBatch: 600\tcost = 0.332664520\tTrain_Acc: 89.17\tTest_Acc: 89.06\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Epoch: 05\tBatch: 060\tcost = 0.358834803\tTrain_Acc: 89.42\tTest_Acc: 89.57\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 120\tcost = 0.403721988\tTrain_Acc: 88.22\tTest_Acc: 88.83\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 180\tcost = 0.318392754\tTrain_Acc: 89.64\tTest_Acc: 90.36\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 240\tcost = 0.313176870\tTrain_Acc: 88.63\tTest_Acc: 89.44\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 300\tcost = 0.255779356\tTrain_Acc: 87.11\tTest_Acc: 88.24\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 360\tcost = 0.151047707\tTrain_Acc: 87.03\tTest_Acc: 87.94\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 420\tcost = 0.355645835\tTrain_Acc: 87.72\tTest_Acc: 88.09\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 480\tcost = 0.306400090\tTrain_Acc: 89.21\tTest_Acc: 89.39\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 540\tcost = 0.485613137\tTrain_Acc: 88.20\tTest_Acc: 89.11\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 05\tBatch: 600\tcost = 0.396911532\tTrain_Acc: 88.11\tTest_Acc: 88.46\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Epoch: 06\tBatch: 060\tcost = 0.140534833\tTrain_Acc: 87.82\tTest_Acc: 88.22\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 120\tcost = 0.211946949\tTrain_Acc: 88.52\tTest_Acc: 88.97\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 180\tcost = 0.294010371\tTrain_Acc: 86.56\tTest_Acc: 87.10\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 240\tcost = 0.302812934\tTrain_Acc: 87.62\tTest_Acc: 88.17\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 300\tcost = 0.267073780\tTrain_Acc: 88.43\tTest_Acc: 88.44\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 360\tcost = 0.231384054\tTrain_Acc: 87.96\tTest_Acc: 88.20\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 420\tcost = 0.215688348\tTrain_Acc: 88.58\tTest_Acc: 88.92\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 480\tcost = 0.186698437\tTrain_Acc: 88.05\tTest_Acc: 88.98\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 540\tcost = 0.093528710\tTrain_Acc: 87.17\tTest_Acc: 87.71\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 06\tBatch: 600\tcost = 0.369535714\tTrain_Acc: 87.80\tTest_Acc: 88.23\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Epoch: 07\tBatch: 060\tcost = 0.171475813\tTrain_Acc: 86.88\tTest_Acc: 87.55\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 120\tcost = 0.289099336\tTrain_Acc: 87.63\tTest_Acc: 87.94\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 180\tcost = 0.143795401\tTrain_Acc: 87.79\tTest_Acc: 88.20\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 240\tcost = 0.240459993\tTrain_Acc: 86.95\tTest_Acc: 87.65\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 300\tcost = 0.513833225\tTrain_Acc: 88.01\tTest_Acc: 88.71\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 360\tcost = 0.262349993\tTrain_Acc: 88.63\tTest_Acc: 88.57\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 420\tcost = 0.190609723\tTrain_Acc: 87.78\tTest_Acc: 88.11\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 480\tcost = 0.229959071\tTrain_Acc: 87.52\tTest_Acc: 88.11\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 540\tcost = 0.351547092\tTrain_Acc: 88.51\tTest_Acc: 88.85\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 07\tBatch: 600\tcost = 0.154178441\tTrain_Acc: 88.87\tTest_Acc: 88.92\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Epoch: 08\tBatch: 060\tcost = 0.369338155\tTrain_Acc: 88.36\tTest_Acc: 88.35\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 120\tcost = 0.235865176\tTrain_Acc: 86.09\tTest_Acc: 86.93\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 180\tcost = 0.194019794\tTrain_Acc: 88.51\tTest_Acc: 88.89\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 240\tcost = 0.322970510\tTrain_Acc: 86.56\tTest_Acc: 87.15\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 300\tcost = 0.510107279\tTrain_Acc: 86.51\tTest_Acc: 86.62\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 360\tcost = 0.238340795\tTrain_Acc: 86.26\tTest_Acc: 86.81\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 420\tcost = 0.210230961\tTrain_Acc: 87.12\tTest_Acc: 87.16\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 480\tcost = 0.271341115\tTrain_Acc: 88.17\tTest_Acc: 88.23\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 540\tcost = 0.126193494\tTrain_Acc: 87.72\tTest_Acc: 87.60\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 08\tBatch: 600\tcost = 0.248805329\tTrain_Acc: 87.56\tTest_Acc: 87.87\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Epoch: 09\tBatch: 060\tcost = 0.439910233\tTrain_Acc: 86.80\tTest_Acc: 87.18\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 120\tcost = 0.355143011\tTrain_Acc: 86.92\tTest_Acc: 87.34\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 180\tcost = 0.423272789\tTrain_Acc: 88.64\tTest_Acc: 88.95\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 240\tcost = 0.287292987\tTrain_Acc: 87.08\tTest_Acc: 87.52\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 300\tcost = 0.252325833\tTrain_Acc: 88.39\tTest_Acc: 88.66\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 360\tcost = 0.292774171\tTrain_Acc: 86.84\tTest_Acc: 87.41\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 420\tcost = 0.153571248\tTrain_Acc: 87.44\tTest_Acc: 87.48\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 480\tcost = 0.346994042\tTrain_Acc: 87.56\tTest_Acc: 87.84\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 540\tcost = 0.325670928\tTrain_Acc: 86.68\tTest_Acc: 86.92\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 09\tBatch: 600\tcost = 0.443470180\tTrain_Acc: 87.86\tTest_Acc: 87.92\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Epoch: 10\tBatch: 060\tcost = 0.139039204\tTrain_Acc: 87.29\tTest_Acc: 87.62\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 120\tcost = 0.329198927\tTrain_Acc: 88.60\tTest_Acc: 88.31\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 180\tcost = 0.333505720\tTrain_Acc: 88.49\tTest_Acc: 88.59\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 240\tcost = 0.400832295\tTrain_Acc: 86.55\tTest_Acc: 87.05\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 300\tcost = 0.291434497\tTrain_Acc: 87.06\tTest_Acc: 86.69\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 360\tcost = 0.192895398\tTrain_Acc: 87.33\tTest_Acc: 87.08\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 420\tcost = 0.184378102\tTrain_Acc: 85.80\tTest_Acc: 86.42\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 480\tcost = 0.124050558\tTrain_Acc: 87.36\tTest_Acc: 87.31\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 540\tcost = 0.160747543\tTrain_Acc: 86.85\tTest_Acc: 86.99\t\tBest_Test_Acc: 90.75\n",
            "Epoch: 10\tBatch: 600\tcost = 0.270772547\tTrain_Acc: 86.36\tTest_Acc: 86.90\t\tBest_Test_Acc: 90.75\n",
            "========================================================================================================================\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}